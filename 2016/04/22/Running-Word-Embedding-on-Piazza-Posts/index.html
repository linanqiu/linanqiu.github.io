<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="Word embedding (very bad explanation follows) is translating each word in a corpus into a D dimension vector. These vectors are supposed to preserve semantic properties and are commonly used in NLP. F">
<meta name="keywords" content="ta,word2vec,nlp,piazza">
<meta property="og:type" content="article">
<meta property="og:title" content="Running Word Embedding on Piazza Posts">
<meta property="og:url" content="http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/index.html">
<meta property="og:site_name" content="Pandamonium">
<meta property="og:description" content="Word embedding (very bad explanation follows) is translating each word in a corpus into a D dimension vector. These vectors are supposed to preserve semantic properties and are commonly used in NLP. F">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://cdn.thinglink.me/api/image/727110550026190849/1240/10/scaletowidth">
<meta property="og:updated_time" content="2016-04-22T07:24:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Running Word Embedding on Piazza Posts">
<meta name="twitter:description" content="Word embedding (very bad explanation follows) is translating each word in a corpus into a D dimension vector. These vectors are supposed to preserve semantic properties and are commonly used in NLP. F">
<meta name="twitter:image" content="https://cdn.thinglink.me/api/image/727110550026190849/1240/10/scaletowidth">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Running Word Embedding on Piazza Posts</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="https://github.com/linanqiu">Projects</a></li>
         
          <li><a href="/Search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2016/05/24/OPT-I-765-Processing-Time/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2016/01/09/Oil-Futures-Curve-Visualization/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&text=Running Word Embedding on Piazza Posts"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&is_video=false&description=Running Word Embedding on Piazza Posts"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Running Word Embedding on Piazza Posts&body=Check out this article: http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&name=Running Word Embedding on Piazza Posts&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#creating-corpus"><span class="toc-number">1.</span> <span class="toc-text">Creating Corpus</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#training-the-model"><span class="toc-number">2.</span> <span class="toc-text">Training the Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#results"><span class="toc-number">3.</span> <span class="toc-text">Results</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Running Word Embedding on Piazza Posts
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Pandamonium</span>
      </span>
      
    <div class="postdate">
        <time datetime="2016-04-22T07:08:07.000Z" itemprop="datePublished">2016-04-22</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/nlp/">nlp</a>, <a class="tag-link" href="/tags/piazza/">piazza</a>, <a class="tag-link" href="/tags/ta/">ta</a>, <a class="tag-link" href="/tags/word2vec/">word2vec</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>Word embedding (very bad explanation follows) is translating each word in a corpus into a D dimension vector. These vectors are supposed to preserve semantic properties and are commonly used in NLP. For example, in a large corpus of words (say the google news corpus), the vector point for man <span class="math inline">\(v\_{king}\)</span> should be near the vector point for <span class="math inline">\(v\_{queen}\)</span>. Furthermore, meaning such as “king is to man as queen is to woman” is preserved via <span class="math inline">\(v\_{king} - v\_{queen} \approx v\_{man} - v\_{queen}\)</span>. These embeddings are generated via neural networks and a common tool for doing this is Word2Vec produced by <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Mikolov et al</a>. A fast implementation of Word2Vec is available in gensim.</p>
<p>For more reading, read <a href="https://www.wikiwand.com/en/Word2vec" target="_blank" rel="noopener">the wiki</a> or the paper itself. For interesting experiments, read these:</p>
<ul>
<li><a href="http://instagram-engineering.tumblr.com/post/117889701472/emojineering-part-1-machine-learning-for-emoji" target="_blank" rel="noopener">Running Word2Vec on Instagram Emojis</a></li>
<li><a href="http://textminingonline.com/training-word2vec-model-on-english-wikipedia-by-gensim" target="_blank" rel="noopener">Word2Vec on English Wiki</a></li>
<li>And, shamelessly, my favorite because I wrote it <a href="https://github.com/linanqiu/word2vec-sentiments" target="_blank" rel="noopener">Sentiment Analysis using Word2Vec</a></li>
</ul>
<p>Won’t it be fun to run this on Piazza posts? I’m a TA for data structures this semester (for Prof Blaer and Prof Bauer) and we have a Piazza containing 800 posts (ripe with answers, followups, etc). Speaking of that, we have an average response time of 5min. Find me another team of TAs that can beat this hurhhurh.</p>
<h1 id="creating-corpus">Creating Corpus</h1>
<p>Turns out Piazza does not have an official API. However, there is an unofficial API. <a href="https://github.com/hfaran/piazza-api" class="uri" target="_blank" rel="noopener">https://github.com/hfaran/piazza-api</a> That’s good enough for us.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> piazza_api <span class="keyword">import</span> Piazza</span><br><span class="line">piazza = Piazza()</span><br><span class="line">piazza.user_login()</span><br></pre></td></tr></table></figure>
<pre><code>Email: lq2137@columbia.edu
········</code></pre>
<p>Now let’s grab all the posts as <code>.json</code>s.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">course = piazza.network(<span class="string">'ijfyurrye2g1oc'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">posts = course.iter_all_posts()</span><br><span class="line"></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> post <span class="keyword">in</span> posts:</span><br><span class="line">    <span class="keyword">if</span> count % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Downloading post %d'</span> % count</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'%d.json'</span> % count, <span class="string">'w'</span>) <span class="keyword">as</span> csv_file:</span><br><span class="line">        csv_file.write(json.dumps(post))</span><br><span class="line">    count += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<pre><code>Downloading post 0
Downloading post 100
Downloading post 200
Downloading post 300
Downloading post 400
Downloading post 500
Downloading post 600
Downloading post 700
Downloading post 800</code></pre>
<p>Then we parse the <code>.json</code>s into a giant <code>.txt</code> by unscrambling the messy <code>.json</code> that the API provides. We also tokenize and perform some basic cleaning (mash everything to lower case). This is a very blunt tool, but should suffice for now.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grab_children</span><span class="params">(post)</span>:</span></span><br><span class="line">    content_children = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">traverse</span><span class="params">(children)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> children:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'history'</span> <span class="keyword">in</span> child:</span><br><span class="line">                history = child[<span class="string">'history'</span>]</span><br><span class="line">                content_children.extend([history_item[<span class="string">'content'</span>] <span class="keyword">for</span> history_item <span class="keyword">in</span> history])</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'children'</span> <span class="keyword">in</span> child:</span><br><span class="line">                traverse(child[<span class="string">'children'</span>])</span><br><span class="line">    traverse(post[<span class="string">'children'</span>])</span><br><span class="line">    <span class="keyword">return</span> content_children</span><br><span class="line"></span><br><span class="line">corpus = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> glob_file <span class="keyword">in</span> glob.glob(<span class="string">'*.json'</span>):</span><br><span class="line">    <span class="keyword">with</span> open(glob_file, <span class="string">'r'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">        post = json.load(json_file)</span><br><span class="line">        content = [history[<span class="string">'content'</span>] <span class="keyword">for</span> history <span class="keyword">in</span> post[<span class="string">'history'</span>]]</span><br><span class="line">        content.extend(grab_children(post))</span><br><span class="line">        content = [BeautifulSoup(text, <span class="string">'html.parser'</span>).get_text() <span class="keyword">for</span> text <span class="keyword">in</span> content]</span><br><span class="line">        content = [word_tokenize(text.lower()) <span class="keyword">for</span> text <span class="keyword">in</span> content]</span><br><span class="line">        corpus.extend(content)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'corpus.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> corpus_file:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> corpus:</span><br><span class="line">        corpus_file.write(<span class="string">'%s\n'</span> % <span class="string">' '</span>.join(line).strip().encode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<h1 id="training-the-model">Training the Model</h1>
<p>Now let’s train the model using <code>gensim</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gensim.models.word2vec <span class="keyword">as</span> word2vec</span><br></pre></td></tr></table></figure>
<p>Turns out <code>gensim</code> has a nice reader that iterates over a text file with one sentence a line. That’s exactly what we produced in the corpus section.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sentences = word2vec.LineSentence(<span class="string">'corpus.txt'</span>)</span><br></pre></td></tr></table></figure>
<p>Now let’s train the model on this corpus. These are some hyperparameters that I found to be good. I shan’t explain them too much, but if you want a little more detail we are essentially using the Skip-Gram with Negative Sampling portion of Word2Vec over 100 iterations and a Skip-Gram window of 15. We also discard all words that occur few than 5 times.</p>
<p>This should take no more than a minute since the corpus is tiny (which theoretically should give us crappy results but let’s see.)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = word2vec.Word2Vec(sentences, min_count=<span class="number">5</span>, workers=<span class="number">8</span>, iter=<span class="number">100</span>, window=<span class="number">15</span>, size=<span class="number">300</span>, negative=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>
<h1 id="results">Results</h1>
<p>Now let’s look at some results.</p>
<p>The <code>most_similar</code> method takes in a word, converts it to its vector representation, and finds the other words that are closest to it by cosine distance. These similar words should have been used in a similar context with the original word. Let’s find some interesting results.</p>
<p>First, let’s do a sanity check using the word <code>homework</code>. Turns out <code>homework</code> is indeed associated with words we’d expect to be associated with <code>homework</code>: <code>solutions</code>, <code>grade</code>, <code>email</code>, and even <code>latex</code>. That’s good.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'homework'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;solutions&#39;, 0.3142701983451843),
 (u&#39;grade&#39;, 0.3114492893218994),
 (u&#39;email&#39;, 0.3077431321144104),
 (u&#39;inserted&#39;, 0.3056461215019226),
 (u&#39;description&#39;, 0.2953464388847351),
 (u&#39;programming&#39;, 0.2912822961807251),
 (u&#39;latex&#39;, 0.2862958312034607),
 (u&#39;theorem&#39;, 0.28351300954818726),
 (u&#39;signature&#39;, 0.2821905314922333),
 (u&#39;posts&#39;, 0.28168296813964844)]</code></pre>
<p>Now for something more advanced. Let’s try the word <code>heap</code>. Turns out we have words that are pretty related to the <code>heap</code> concept:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'heap'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;build&#39;, 0.30242919921875),
 (u&#39;percolatedown&#39;, 0.2909688353538513),
 (u&#39;one-by-one&#39;, 0.28867167234420776),
 (u&#39;deletion&#39;, 0.2867096960544586),
 (u&#39;k&#39;, 0.28530699014663696),
 (u&#39;discussed&#39;, 0.2748313546180725),
 (u&#39;linear&#39;, 0.26605361700057983),
 (u&#39;increasing&#39;, 0.2620879113674164),
 (u&#39;quicksort&#39;, 0.25420695543289185),
 (u&#39;instructions&#39;, 0.2464582324028015)]</code></pre>
<p>Now for the most awesome result ever. What concepts are associated with <code>good</code>? Turns out I’m one of them.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'good'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;great&#39;, 0.35192880034446716),
 (u&#39;very&#39;, 0.3159177303314209),
 (u&#39;almost&#39;, 0.30515187978744507),
 (u&#39;concepts&#39;, 0.3004434108734131),
 (u&#39;linan&#39;, 0.2995752990245819),
 (u&#39;too&#39;, 0.2964840829372406),
 (u&#39;basic&#39;, 0.2916448712348938),
 (u&#39;fit&#39;, 0.28502458333969116),
 (u&#39;useful&#39;, 0.2824944257736206),
 (u&#39;answered&#39;, 0.28173112869262695)]</code></pre>
<div class="figure">
<img src="https://cdn.thinglink.me/api/image/727110550026190849/1240/10/scaletowidth" alt="doge">
<p class="caption">doge</p>
</div>
<p>Let’s see what’s similar to the profs. Turns out Prof Blaer’s love for <code>ocaml</code> is well noted.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'blaer'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;professor&#39;, 0.7219115495681763),
 (u&#39;prof.&#39;, 0.6643839478492737),
 (u&#39;bauer&#39;, 0.4823228120803833),
 (u&#39;today&#39;, 0.44543683528900146),
 (u&#39;went&#39;, 0.4129117727279663),
 (u&#39;pm&#39;, 0.4097597897052765),
 (u&#39;session&#39;, 0.40625864267349243),
 (u&#39;mentioned&#39;, 0.4010222554206848),
 (u&#39;ocaml&#39;, 0.3884058892726898),
 (u&#39;tonight&#39;, 0.38226139545440674)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'bauer'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;professor&#39;, 0.6429691314697266),
 (u&#39;slides&#39;, 0.5277301669120789),
 (u&#39;prof.&#39;, 0.5086715221405029),
 (u&#39;lecture&#39;, 0.4928019642829895),
 (u&#39;blaer&#39;, 0.4823228120803833),
 (u&#39;sign&#39;, 0.4362599849700928),
 (u&#39;pm&#39;, 0.3718754053115845),
 (u&#39;went&#39;, 0.3549380302429199),
 (u&#39;tonight&#39;, 0.326946496963501),
 (u&#39;perform&#39;, 0.3159770965576172)]</code></pre>
<p>Sasha loves grades, which is unsurprising.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(<span class="string">'sasha'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[(u&#39;email&#39;, 0.5593311786651611),
 (u&#39;ta&#39;, 0.4426960051059723),
 (u&#39;monday&#39;, 0.4238441586494446),
 (u&#39;grade&#39;, 0.42380213737487793),
 (u&#39;talk&#39;, 0.40258437395095825),
 (u&#39;grading&#39;, 0.3625785708427429),
 (u&#39;mentioned&#39;, 0.3540249466896057),
 (u&#39;approaches&#39;, 0.3509276807308197),
 (u&#39;cs&#39;, 0.3439938724040985),
 (u&#39;hope&#39;, 0.3431258201599121)]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.doesnt_match(<span class="string">"bauer blaer graph"</span>.split())</span><br></pre></td></tr></table></figure>
<pre><code>&#39;graph&#39;</code></pre>
<p>And of course it differentiates between a TA and a professor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.doesnt_match(<span class="string">"bauer blaer linan"</span>.split())</span><br></pre></td></tr></table></figure>
<pre><code>&#39;linan&#39;</code></pre>
<p>ISN’T THIS FUCKING AWESOME.</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="https://github.com/linanqiu">Projects</a></li>
         
          <li><a href="/Search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#creating-corpus"><span class="toc-number">1.</span> <span class="toc-text">Creating Corpus</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#training-the-model"><span class="toc-number">2.</span> <span class="toc-text">Training the Model</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#results"><span class="toc-number">3.</span> <span class="toc-text">Results</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&text=Running Word Embedding on Piazza Posts"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&is_video=false&description=Running Word Embedding on Piazza Posts"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Running Word Embedding on Piazza Posts&body=Check out this article: http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&title=Running Word Embedding on Piazza Posts"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://linanqiu.github.io/2016/04/22/Running-Word-Embedding-on-Piazza-Posts/&name=Running Word Embedding on Piazza Posts&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2018 Linan Qiu
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="https://github.com/linanqiu">Projects</a></li>
         
          <li><a href="/Search/">Search</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


